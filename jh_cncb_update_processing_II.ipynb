{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute variant associated IR and FR (normalization: v4 with lags) \n",
    "### Final step\n",
    "### Update\n",
    "\n",
    "### Import IR, PP and lagged_FR - assemble them into a single file each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from scipy.signal import lfilter\n",
    "import pickle as pickle\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required files - lagged FR\n",
    "dates = pd.read_table('dates.txt',header=None,index_col=0)\n",
    "old_date = dates.loc['old_date'].values[0]\n",
    "new_date = dates.loc['new_date'].values[0]\n",
    "date_idx = pd.read_csv('dateindex_splits.csv',names=['s','e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ALL OUTPUTS YOU GENERATED\n",
    "lagged_FR_init = {}\n",
    "\n",
    "for i in range(len(date_idx)):\n",
    "    with open(f\"../Input_files/lagged_FR{i+1}_{new_date.replace('_','')}.pkl\",'rb') as file:\n",
    "        lagged_FR_init[i] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.06it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.03it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dates=list(lagged_FR_init[0].keys())\n",
    "\n",
    "lagged_FR=lagged_FR_init[0]\n",
    "\n",
    "for i in range(len(date_idx)):\n",
    "    for date in tqdm(list_dates[int(date_idx.iloc[i]['s']):int(date_idx.iloc[i]['e'])]):\n",
    "\n",
    "        for var in lagged_FR[date].keys():\n",
    "            lagged_FR[date][var]=lagged_FR_init[i][date][var]  \n",
    "\n",
    "# If you have more than 2 partial files, keep adding them to the aggregate list - for example, supposing you have two more partial files lagged_FR3 and lagged_FR4:\n",
    "\n",
    "# for date in tqdm(list_dates[717:730]):   # CHANGE DATES\n",
    "#    for var in lagged_FR[date].keys():#\\n\",\n",
    "#        lagged_FR[date][var]=lagged_FR3[date][var]\n",
    "\n",
    "# for date in tqdm(list_dates[730:745]):  # CHANGE DATES\n",
    "#     for var in lagged_FR[date].keys():#\\n\",\n",
    "#         lagged_FR[date][var]=lagged_FR4[date][var]\n",
    "\n",
    "del lagged_FR_init # delete all partial files\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR\n",
    "\n",
    "# LOAD ALL OUTPUTS YOU GENERATED\n",
    "IR_init = {}\n",
    "\n",
    "for i in range(len(date_idx)):\n",
    "    with open(f\"../Input_files/IR{i+1}_{new_date.replace('_','')}.pkl\",'rb') as file:\n",
    "        IR_init[i] = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:04<00:00,  3.08it/s]\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.33it/s]\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.40it/s]\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.47it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IR=IR_init[0]\n",
    "\n",
    "for i in range(len(date_idx)):\n",
    "    for date in tqdm(list_dates[int(date_idx.iloc[i]['s']):int(date_idx.iloc[i]['e'])]):\n",
    "        for var in IR[date].keys():\n",
    "            IR[date][var]=IR_init[i][date][var]\n",
    "        \n",
    "# SAME AS ABOVE - EXTEND TO ALL PARTIAL FILES AS NEEDED\n",
    "        \n",
    "del IR_init\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP\n",
    "\n",
    "# LOAD ALL OUTPUTS YOU GENERATED\n",
    "PP_init = {}\n",
    "\n",
    "for i in range(len(date_idx)):\n",
    "    with open(f\"../Input_files/PP{i+1}_{new_date.replace('_','')}.pkl\",'rb') as file:\n",
    "        PP_init[i] = pickle.load(file)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP=PP_init[0]\n",
    "\n",
    "# SAME AS ABOVE - EXTEND TO ALL PARTIAL FILES AS NEEDED\n",
    "\n",
    "for date in tqdm(list_dates[int(date_idx.iloc[i]['s']):int(date_idx.iloc[i]['e'])]):\n",
    "    for var in PP[date].keys():\n",
    "        PP[date][var]=PP_init[i][date][var]\n",
    "\n",
    "del PP_init\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "\n",
    "# CHANGE DATES\n",
    "with open(f\"../Input_files/lagged_FR_{old_date}_{new_date}.pkl\",'wb') as file:\n",
    "    pickle.dump(lagged_FR,file,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# CHANGE DATES\n",
    "with open(f\"../Input_files/IR_{old_date}_{new_date}.pkl\",'wb') as file:\n",
    "    pickle.dump(IR,file,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# CHANGE DATES\n",
    "with open(f\"../Input_files/PP_{old_date}_{new_date}.pkl\",'wb') as file:\n",
    "    pickle.dump(PP,file,protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you just need to load the files before for the final table writing step, uncomment this cell and execute\n",
    "\n",
    "with open(f\"../Input_files/lagged_FR_{old_date}_{new_date}.pkl\",'rb') as file:\n",
    "   lagged_FR = pickle.load(file)\n",
    "    \n",
    "with open(f\"../Input_files/IR_{old_date}_{new_date}.pkl\",'rb') as file:\n",
    "   IR = pickle.load(file)\n",
    "    \n",
    "with open(f\"../Input_files/PP_{old_date}_{new_date}.pkl\",'rb') as file:\n",
    "   PP = pickle.load(file)\n",
    "    \n",
    "list_dates=list(lagged_FR.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Input_files/aggregated_mutations_window00_{dt.strftime(dt.strptime(old_date,'%m_%d_%y') + timedelta(days=1),'%m_%d_%y')}_{new_date}.pkl\",'rb') as file:\n",
    "    aggregated_mutations_window = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [1:41:45<00:00, 95.40s/it] \n"
     ]
    }
   ],
   "source": [
    "# Finally, assemble and save the output tables for GPR.\n",
    "\n",
    "# CHANGE INDEXES - these will be the same as in jh_cncb_update_processing_I.py, line 58 (AFTER YOU UPDATED THE INDEXES TO CURRENT UPDATE)\n",
    "for date in tqdm(list_dates[int(date_idx.iloc[0]['s']):int(date_idx.iloc[-1]['e'])]):\n",
    "    unique_muts = pd.DataFrame(list(aggregated_mutations_window[date].keys()),columns=['descriptor'])\n",
    "    unique_muts = pd.DataFrame.join(unique_muts,pd.DataFrame(unique_muts['descriptor'].str.split(',').to_list())) #assign columns with parsed descriptor\n",
    "    unique_muts.set_index('descriptor',inplace=True)\n",
    "    unique_muts[0] = pd.to_numeric(unique_muts[0])\n",
    "    unique_muts[1] = pd.to_numeric(unique_muts[1])\n",
    "    unique_muts.sort_values([0,1],inplace=True)\n",
    "    unique_muts.columns = ['Start','End','Ref','Alt','VEP','Variant Type','Ref_AA','Alt_AA','AA']\n",
    "\n",
    "    counts = {}\n",
    "    num_countries = {}\n",
    "    infection_rate = {}\n",
    "    fatality_rate = {}\n",
    "    counted_countries = {}\n",
    "\n",
    "    for desc in unique_muts.index:\n",
    "        counts[desc] = len(aggregated_mutations_window[date][desc])\n",
    "        num_countries[desc] = len(set(aggregated_mutations_window[date][desc]))\n",
    "        ir_sum=sum(IR[date][desc])\n",
    "        pp_sum=sum(PP[date][desc])\n",
    "        if pp_sum==0:\n",
    "            infection_rate[desc] = ir_sum\n",
    "        else:\n",
    "            infection_rate[desc] = ir_sum/pp_sum #/counts[desc]\n",
    "        #infection_rate[desc] = np.mean(aggregated_mutations_window[date][desc][1]) #since accumulated over countries before denomenator of mean is wrong\n",
    "        if counts[desc]==0:\n",
    "            fatality_rate[desc] = sum(lagged_FR[date][desc])\n",
    "        else:\n",
    "            fatality_rate[desc] = sum(lagged_FR[date][desc])/counts[desc]#/counts[desc]\n",
    "        #fatality_rate[desc] = np.mean(aggregated_mutations_window[date][desc][2]) #since accumulated over countries before denomenator of mean is wrong\n",
    "        counted_countries[desc] = dict(Counter(aggregated_mutations_window[date][desc]))\n",
    "\n",
    "    unique_muts['counts'] = unique_muts.index.to_series().map(counts)\n",
    "    unique_muts['countries'] = unique_muts.index.to_series().map(num_countries)\n",
    "    unique_muts['infection_rate'] = unique_muts.index.to_series().map(infection_rate)\n",
    "    unique_muts['fatality_rate'] = unique_muts.index.to_series().map(fatality_rate)\n",
    "    unique_muts['counted_countries'] = unique_muts.index.to_series().map(counted_countries)\n",
    "    unique_muts.index = unique_muts.index.str.split(',').str[0:4].str.join('_')\n",
    "    unique_muts.sort_values('counts',ascending=False)\n",
    "    if not os.path.exists(f'../Output Files/cumulative_daily_AF_v4_lag_{new_date}'):\n",
    "        os.mkdir(f'../Output Files/cumulative_daily_AF_v4_lag_{new_date}')\n",
    "    # CREATE A FOLDER FOR THE OUTPUT IN ../Output Files: cumulative_daily_AF_v4_lag_<your date>\n",
    "    # CHANGE FOLDER NAME BELOW TO THE FOLDER YOU CREATED\n",
    "    unique_muts.to_csv(f\"../Output Files/cumulative_daily_AF_v4_lag_{new_date}/{date.strftime('%m_%d_%y')}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
